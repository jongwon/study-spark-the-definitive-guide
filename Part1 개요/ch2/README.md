# 2. 간단히 살펴보기

## 2.1 기본 아키텍처

#### 스파크 애플리케이션

- 드라이버 프로세스 : 클러스터 노드중 하나에서 실행되는(클러스터 모드인 경우) 메인 프로세스로 스파크 애플리케이 정보의 유지와 관리, 분석, 배포, 스케쥴링등을 모두 관장함.
- Executor 프로세스 : 드라이버가 할당한 코드를 수행해서 작업하고 드라이버에게 보고함.
- 클러스터 매니저 : 스파크(스탠드 얼론), MESOS, YARN 중에 하나로, 사용 가능한 자원을 파악해 자원을 할당하고 드라이버와 Executor를 실행시켜 줌.

## 2.2 다양한 언어 API

- 스칼라 : 스파크 기본 언어. jar로 빌드
- 자바 : 자바 지원. jar 빌드
- 파이썬 : 거의 모든 구조를 지원. py 스크립트로 배포
- SQL : 비프로그래머를 위해 제공
- R : 스파크 core에 포함된 SparkR과 R 커뮤니티 패키지인 sparklyr이 있음. (32장에 설명)

어느 언어를 쓰든 스파크 코드의 드라이버는 Java로 컴파일되어 JVM 위에서 돌아가고, 다른 언어를 쓸 경우 컴파일된 코드가 SparkSession 과 통신하는 구조를 가지고 있다. (스파크 자체가 분산 작업에 계획하고 실행코드를 작성하는 컴파일러의 성격을 가지고 있다.)

```
다만 Executor는 다른 언어의 라이브러리로 동작할 수 있는 것으로 보인다.
```

## 2.3 스파크 API

다양한 언어로 스파크를 사용할 수 있는 이유는 저수준의 비구조적 API와 고수준의 구조적 API 두가지를 제공하기 때문이다.

- 저수준 API(비구조적) : RDD, SparkContext, 분산형 공유 변수(accumulator, broadcast 변수)
- 고수준 API(구조적) : Dataset, DataFrame, SQL 테이블/뷰

## 2.4 시작하기

스파크 애플리케이션을 개발하려면 대화형 콘솔창을 실행하거나(spark-shell, pyspark) 스파크 애플리케이션을 클러스터 매니저에게 전송해야 한다.

## 2.5 SparkSession




## 2.6 DataFrame

## 2.7 트랜스포메이션

* 스파크의 데이터들은 기본적으로 immutable 이다. 이 말은 한번 생성하면 수정이 불가하고 삭제와 저장만 가능하다는 말이다.
* DataFrame 을 변경하려면 데이터를 변환(Transformation, 트렌스포메이션) 해야 한다.
``` scala
val myRange = spark.range(1000).toDF("number")
val divisBy2 = myRange.where("number % 2 = 0")
```  

트렌스포메이션은 두가지 유형이 있다.
* 좁은(narrow) 의존성
  * 입력 파티션이 하나의 출력 파티션에만 영향을 주는 것 
  * 입력과 출력 사이에 데이터의 파이프라이닝이 자동으로 수행된다.
  * 메모리에서 수행된다.
* 넓은(wide) 의존성
  * 입력 파티션인 둘 이상의 여러 출력 파티션에 영향을 주는 것.
  * 셔플이 수행된다.
  * 출력을 디스크에 저장한다. (속도가 느려진다???)


## 2.8 액션

지연 연산(lazy evaluation) 
* 연산 명령이 내려지는 즉시 수행하는 것이 아니라,
* 코드를 실행해야 하는 마지막 순간까지 대기하면서 데이터의 흐름을 취적화 하도록 실행 계획을 만든다.
* DataFrame의 조건절 푸시다운(?)이 한가지 예가 될 수 있다.

## 2.9 스파크 UI

## 2.10 종합 예제

## 2.11 정리
